{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removed the Header of Sentence Database and Footer of Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_GT_XML_IAM(input_directory, output_directory): \n",
    "    files = files = [f for f in os.listdir(input_directory) if f.endswith('.txt')]\n",
    "    #print(files\n",
    "    #files = [\"ocr_a06-114.txt\"]\n",
    "    unmatched = []\n",
    "    for file in files: \n",
    "        file_path = input_directory + file\n",
    "        file_open = open(file_path,'r')\n",
    "        contents = file_open.read()\n",
    "        #dynamic_part = r\"[A-Za-z]\\s*\\d{2}[-\\s]*\\d{3}\"\n",
    "        start_marker = r\"Sentence Database(\\s?\\;?\\.?\\|?\\'?\\:?\\s?[A-Za-z]{1,3}?\\d{0,2}?[A-Za-z]{0,3}?\\d{1,2}?-\\d{3}?)?\"\n",
    "        # start_marker = \"^Sentence Database .?.?\" + file[5:][:-4]\n",
    "        #start_marker = r\"^Sentence\\s*Database\\s*\" + \".?\" + file[5:][:-4]\n",
    "        #print(\"this is the start marker\",start_marker)\n",
    "        match_header = re.search(start_marker, contents.strip(),re.DOTALL)\n",
    "        #print('this is the match marker',match_header)\n",
    "        if match_header: \n",
    "            #print(match_header)\n",
    "            content_n_header = contents[match_header.end() + 2:]\n",
    "            #print(content_n_header)\n",
    "            end_marker = \"Name:.*\"\n",
    "            match_footer = re.search(end_marker, content_n_header, flags=re.DOTALL)\n",
    "            #print(match_footer)\n",
    "            if match_footer:\n",
    "                contents_n_header_footer = content_n_header[:match_footer.start()]\n",
    "                #print(contents_n_header_footer)\n",
    "            file_open.close()\n",
    "            text_name = output_directory + \"cleaned_\" + file[:-4] + \".txt\"\n",
    "            with open(text_name,'w') as f: \n",
    "                f.write(contents_n_header_footer)\n",
    "        else: \n",
    "            unmatched.append(file)\n",
    "    return unmatched \n",
    "        \n",
    "\n",
    "AD_no = run_GT_XML_IAM('OCR/completed-OCR/IAM/formsAD/',\"OCR/completed-OCR/IAM_cleaned/formsAD/\")\n",
    "EH_no = run_GT_XML_IAM('OCR/completed-OCR/IAM/formsEH/',\"OCR/completed-OCR/IAM_cleaned/formsEH/\")\n",
    "lZ_no = run_GT_XML_IAM('OCR/completed-OCR/IAM/formsIZ/',\"OCR/completed-OCR/IAM_cleaned/formsIZ/\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seperating the GT from the Handwritten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/homebrew/anaconda3/lib/python3.12/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /opt/homebrew/anaconda3/lib/python3.12/site-packages (from nltk) (4.66.5)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/amandanitta/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('words')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.corpus import words\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "# Load a dictionary of English words\n",
    "english_words = set(words.words())\n",
    "\n",
    "def is_coherent(sentence):\n",
    "    \"\"\"Heuristic to check if a sentence is coherent based on valid word percentage.\"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    valid_words = [word for word in tokens if re.sub(r\"[^a-zA-Z]\", \"\", word).lower() in english_words]\n",
    "    coherence_ratio = len(valid_words) / max(len(tokens), 1)  # Avoid division by zero\n",
    "    return coherence_ratio > 0.1 # Threshold can be adjusted\n",
    "\n",
    "def split_text_by_coherence(text):\n",
    "    \"\"\"\n",
    "    Splits text into 'ground truth' and 'guessed' sections based on coherence.\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    ground_truth = []\n",
    "    guessed = []\n",
    "    in_guessed_section = False\n",
    "\n",
    "    for sentence in sentences:\n",
    "        if is_coherent(sentence):\n",
    "            if in_guessed_section:  # Once we've moved to guessed, keep adding there\n",
    "                guessed.append(sentence)\n",
    "            else:\n",
    "                ground_truth.append(sentence)\n",
    "        else:\n",
    "            guessed.append(sentence)\n",
    "            in_guessed_section = True  # Switch to guessed once we encounter incoherent text\n",
    "\n",
    "    #return \" \".join(ground_truth).strip(), \" \".join(guessed).strip()\n",
    "    return \" \".join(guessed).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_all_files(input_directory):\n",
    "    #files = files = [f for f in os.listdir(input_directory) if f.endswith('.txt')]\n",
    "    #print(files\n",
    "    files = [\"cleaned_ocr_e04-004.txt\"]\n",
    "    for file in files: \n",
    "        file_path = input_directory + file\n",
    "        file_open = open(file_path,'r')\n",
    "        contents = file_open.read()\n",
    "        print(contents)\n",
    "        print(\"------------------------------------------------------------------------------\")\n",
    "        print(split_text_by_coherence(contents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start by making the two side frames from 1 in. by 1 1/4 in. planed timber as shown\n",
      "in Fig. 1. Although the timber will have already been machine planed, remember to\n",
      "go over each piece with a smoothing plane, otherwise the marks left by the cutters\n",
      "will show up after painting. The dimensions given enable the feeding tray to slide\n",
      "over a 30 in. table, but the height can be altered if required.\n",
      "\n",
      "Stat by watig the two side frames feom 4 én. by 1 1Men. planed\n",
      "tuber #S shown ian Fig. 4. Although the timber well have alacady\n",
      "bun machine planed, wouduber ty go one tach péece wth\n",
      "x smoothing plane, otherwise. the wacks Uf by He aptes\n",
      "wilh show up after paiutting. The dimenstons pen cmble the\n",
      "feed Tray to slide ovec « 30 ul. tuble, but the hetehrt\n",
      "\n",
      "con be abled it required.\n",
      "\n",
      "\n",
      "------------------------------------------------------------------------------\n",
      "1. Although the timber will have already been machine planed, remember to\n",
      "go over each piece with a smoothing plane, otherwise the marks left by the cutters\n",
      "will show up after painting. The dimensions given enable the feeding tray to slide\n",
      "over a 30 in. table, but the height can be altered if required. Stat by watig the two side frames feom 4 én. by 1 1Men. planed\n",
      "tuber #S shown ian Fig. 4. Although the timber well have alacady\n",
      "bun machine planed, wouduber ty go one tach péece wth\n",
      "x smoothing plane, otherwise. the wacks Uf by He aptes\n",
      "wilh show up after paiutting. The dimenstons pen cmble the\n",
      "feed Tray to slide ovec « 30 ul. tuble, but the hetehrt\n",
      "\n",
      "con be abled it required.\n"
     ]
    }
   ],
   "source": [
    "extract_all_files('OCR/completed-OCR/IAM_cleaned/formsEH/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
