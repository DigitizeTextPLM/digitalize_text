{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/nittaak/661'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os \n",
    "\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import ConcatDataset\n",
    "from transformers import PreTrainedTokenizerFast, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from cosinesimcontained import get_cosine_sim_bert\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer, BertModel\n",
    "\n",
    "def load_data(data_dir, labels_dir):\n",
    "    data_files = sorted([f for f in os.listdir(data_dir) if f.endswith('.txt')])\n",
    "    label_files = sorted([f for f in os.listdir(labels_dir) if f.endswith('.txt')])\n",
    "\n",
    "    dataset = []\n",
    "\n",
    "    for data_file, label_file in zip(data_files, label_files):\n",
    "        with open(os.path.join(data_dir, data_file), 'r') as df, open(os.path.join(labels_dir, label_file), 'r') as lf:\n",
    "            data = df.read()\n",
    "            label = lf.read()\n",
    "\n",
    "            dataset.append((data, label))  # Append as a tuple (data, label)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, data, labels, transform=None):\n",
    "        self.data = data\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self.data[idx]\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        if self.transform:\n",
    "            data = self.transform(data)\n",
    "\n",
    "        return data, label\n",
    "\n",
    "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "# Train Data\n",
    "dataset_betham_train = load_data('/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/train/Betham/OCR', '/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/train/Betham/GT/')\n",
    "dataset_IAM_train = load_data('/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/train/IAM/OCR/','/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/train/IAM/GT/')\n",
    "\n",
    "## Validation Data\n",
    "dataset_betham_val = load_data('/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/validation/Betham/OCR', '/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/validation/Betham/GT/')\n",
    "dataset_IAM_val = load_data('/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/validation/IAM/OCR/','/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/validation/IAM/GT/')\n",
    "\n",
    "## Test Data\n",
    "dataset_betham_test = load_data('/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/test/Betham/OCR/', '/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/test/Betham/GT/')\n",
    "dataset_IAM_test = load_data('/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/test/IAM/OCR/','/Users/amandanitta/Desktop/GitHub/ICS661/ICS661-proj/digitalize_handwritten/dataForModel/test/IAM/GT/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "len_data_bentham = []\n",
    "len_labels_bentham = []\n",
    "\n",
    "len_data_IAM = []\n",
    "len_labels_IAM = []\n",
    "\n",
    "for data, label in dataset_betham_val:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    len_data_bentham.append(len(data))\n",
    "    len_labels_bentham.append(len(label))\n",
    "\n",
    "for data, label in dataset_IAM_val:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    len_data_IAM.append(len(data))\n",
    "    len_labels_IAM.append(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in dataset_betham_train:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    len_data_bentham.append(len(data))\n",
    "    len_labels_bentham.append(len(label))\n",
    "\n",
    "for data, label in dataset_IAM_train:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    len_data_IAM.append(len(data))\n",
    "    len_labels_IAM.append(len(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "for data, label in dataset_betham_test:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    len_data_bentham.append(len(data))\n",
    "    len_labels_bentham.append(len(label))\n",
    "\n",
    "for data, label in dataset_IAM_test:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    len_data_IAM.append(len(data))\n",
    "    len_labels_IAM.append(len(label))\n",
    "\n",
    "len_overall_handwritten = len_labels_bentham + len_labels_IAM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.statology.org/confidence-interval-for-median/ \n",
    "\n",
    "\"\"\"\n",
    "Lower: j = median - z*sqrt(1-q)\n",
    "Upper: k = median + z*sqrt(1-q)\n",
    "\"\"\"\n",
    "z_value = 1.96\n",
    "quartile = 0.5\n",
    "total_IAM = len(len_labels_IAM)\n",
    "\n",
    "confidence_IAM = z_value*np.sqrt(total_IAM*quartile*(1-quartile))\n",
    "len_labels_IAM = sorted(len_labels_IAM)\n",
    "confidence_left_IAM = len_labels_IAM[-int(np.floor(confidence_IAM))]\n",
    "confidence_right_IAM = len_labels_IAM[int(np.floor(confidence_IAM))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_value = 1.96\n",
    "quartile = 0.5\n",
    "total_bentham = len(len_labels_bentham)\n",
    "\n",
    "confidence_bentham = z_value*np.sqrt(total_bentham*quartile*(1-quartile))\n",
    "len_labels_bentham = sorted(len_labels_bentham)\n",
    "confidence_left_bentham = len_labels_bentham[-int(np.floor(confidence_bentham))]\n",
    "confidence_right_bentham = len_labels_bentham[int(np.floor(confidence_bentham))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "z_value = 1.96\n",
    "quartile = 0.5\n",
    "total_overall = len(len_overall_handwritten)\n",
    "\n",
    "confidence_overall = z_value*np.sqrt(total_overall*quartile*(1-quartile))\n",
    "len_overall_handwritten = sorted(len_overall_handwritten)\n",
    "confidence_left_overall = len_overall_handwritten[-int(np.floor(confidence_overall))]\n",
    "confidence_right_overall = len_overall_handwritten[int(np.floor(confidence_overall))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IAM Dataset\n",
      "Min: 58\n",
      "Max: 903\n",
      "Median 375.0\n",
      "Total Documents 1251\n",
      "Confidence: [266,564]\n",
      "-------------------------\n",
      "\n",
      "Bentham Dataset\n",
      "Min: 133\n",
      "Max: 3452\n",
      "Median: 1160.0\n",
      "Total Documents: 473\n",
      "Confidence: [595,1896]\n",
      "-------------------------\n",
      "\n",
      "Overall Dataset\n",
      "Min: 58\n",
      "Max: 3452\n",
      "Median: 409.0\n",
      "Total Documents: 1724\n",
      "Confidence: [408, 777]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(f'IAM Dataset')\n",
    "print(f'Min: {np.min(len_labels_IAM)}')\n",
    "print(f'Max: {np.max(len_labels_IAM)}')\n",
    "print(f'Median {np.median(len_labels_IAM)}')\n",
    "print(f'Total Documents {len(len_labels_IAM)}')\n",
    "print(f'Confidence: [{confidence_right_IAM},{confidence_left_IAM}]')\n",
    "print(\"-------------------------\\n\")\n",
    "\n",
    "print('Bentham Dataset')\n",
    "print(f'Min: {np.min(len_labels_bentham)}')\n",
    "print(f'Max: {np.max(len_labels_bentham)}')\n",
    "print(f'Median: {np.median(len_labels_bentham)}')\n",
    "print(f'Total Documents: {len(len_labels_bentham)}')\n",
    "print(f'Confidence: [{confidence_right_bentham},{confidence_left_bentham}]')\n",
    "\n",
    "print(\"-------------------------\\n\")\n",
    "print('Overall Dataset')\n",
    "print(f'Min: {np.min(len_overall_handwritten)}')\n",
    "print(f'Max: {np.max(len_overall_handwritten)}')\n",
    "print(f'Median: {np.median(len_overall_handwritten)}')\n",
    "print(f'Total Documents: {len(len_overall_handwritten)}')\n",
    "print(f'Confidence: [{confidence_left_overall}, {confidence_right_overall}]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_sim_val = []\n",
    "count_val = 0\n",
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "\n",
    "for data, label in dataset_betham_val:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    cosine_sim_val.append( get_cosine_sim_bert(data, label, tokenizer=tokenizer, model=model,device=device))\n",
    "    count_val += 1\n",
    "\n",
    "\n",
    "for data, label in dataset_IAM_val:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    cosine_sim_val.append( get_cosine_sim_bert(data, label, tokenizer=tokenizer, model=model,device=device))\n",
    "    count_val += 1\n",
    "\n",
    "cosine_sim_val_avg = np.mean(cosine_sim_val[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity validation average: 0.7338921888396631\n"
     ]
    }
   ],
   "source": [
    "print(f'cosine similarity validation average: {cosine_sim_val_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model.to(device)\n",
    "\n",
    "cosine_sim_test = []\n",
    "count_test = 0\n",
    "\n",
    "for data, label in dataset_betham_test:\n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    cosine_sim_test.append( get_cosine_sim_bert(data, label, tokenizer=tokenizer, model=model,device=device))\n",
    "\n",
    "for data, label in dataset_IAM_test: \n",
    "    if data is None or label is None: \n",
    "        continue\n",
    "    if len(data) == 0 or len(label) == 0: \n",
    "        continue\n",
    "    cosine_sim_test.append( get_cosine_sim_bert(data, label, tokenizer=tokenizer, model=model,device=device))\n",
    "    \n",
    "cosine_sim_test_avg = np.mean(cosine_sim_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cosine similarity test average: 0.7383061489172741\n"
     ]
    }
   ],
   "source": [
    "print(f'cosine similarity test average: {cosine_sim_test_avg}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
